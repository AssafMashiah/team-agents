# =============================================================================
# OpenClaw Team Agents — Environment Variables
# =============================================================================
# Copy this file to .env and fill in your values.
# NEVER commit .env to git.
# =============================================================================

# --- AI Providers -----------------------------------------------------------
# Anthropic (Claude) — primary recommended provider
ANTHROPIC_API_KEY=sk-ant-api03-YOUR_KEY_HERE

# OpenAI (optional fallback)
# OPENAI_API_KEY=sk-YOUR_KEY_HERE

# OpenRouter (optional, access to many models including open-source)
# OPENROUTER_API_KEY=sk-or-YOUR_KEY_HERE

# --- OpenClaw Gateway -------------------------------------------------------
# Token to authenticate the Control UI (http://localhost:18789?token=...)
# Generate with: openssl rand -hex 32
OPENCLAW_GATEWAY_TOKEN=REPLACE_WITH_RANDOM_32_CHAR_HEX

# Ports (change if 18789/18790 are taken on your machine)
OPENCLAW_PORT=18789
OPENCLAW_BRIDGE_PORT=18790

# --- Storage Paths ----------------------------------------------------------
# Where OpenClaw stores config, memory, and agent state on the host
OPENCLAW_CONFIG_DIR=~/.openclaw

# The working directory accessible to the agent
OPENCLAW_WORKSPACE_DIR=./workspace

# --- Discord Integration ----------------------------------------------------
# Create a bot at https://discord.com/developers/applications
# Required permissions: Send Messages, Read Message History, View Channels
DISCORD_BOT_TOKEN=
DISCORD_GUILD_ID=
# Comma-separated channel IDs the bot is allowed to respond in (leave empty = all channels)
DISCORD_CHANNEL_ALLOWLIST=

# --- Signal Integration (optional — start with: docker compose --profile signal up -d)
# Your Signal phone number in international format: +12125551234
SIGNAL_PHONE_NUMBER=
SIGNAL_API_PORT=8080

# --- Ollama Local LLM -------------------------------------------------------
OLLAMA_PORT=11434
# Model to pull on first setup. Options:
#   llama3.2:3b   — fast, ~2GB RAM needed
#   llama3.1:8b   — balanced (recommended default)
#   llama3.3:70b  — most powerful, needs 40GB+ RAM
#   mistral:7b    — fast reasoning and coding
#   codellama:13b — optimised for code
OLLAMA_DEFAULT_MODEL=llama3.1:8b

# --- Cloudflare Tunnel (remote/CF deployment only) --------------------------
# Get from: https://one.dash.cloudflare.com -> Networks -> Tunnels
CLOUDFLARE_TUNNEL_TOKEN=

# Remote Ollama host (for CF deployment where local Ollama isn't available)
# E.g. your home server: http://home.yourdomain.com:11434
REMOTE_OLLAMA_HOST=
